# Introduction

The first step of MBO requires an initial set of evaluation points which is then evaluated by the black box function.
The basic procedure of MBO is an iterating loop of the following steps:

1. A user defined surrogate model is fitted on the evaluated points
2. A new evaluation point is proposed by an infill criterion 
3. Its performance is evaluated

Once again, take a look at the already introduced basic example of the optimization of the one dimensional Rastrigin function.

```{r}
library(mlrMBO)
obj.fun = makeRastriginFunction(1)

learner = makeLearner("regr.km", predict.type = "se", covtype = "matern3_2")
control = makeMBOControl()
control = setMBOControlTermination(control, iters = 5)
control = setMBOControlInfill(control, crit = "ei", opt = "ea")

result = mbo(obj.fun, learner = learner, control = control, show.info = TRUE)
```

From this example we can easily recognize some **mlrMBO** essentials like parameters, learners and the control object.

Basically the following steps are needed to start a surrogate-based optimization with our package. 
Each step ends with an R object, which is then passed to ```mbo()```, i. e., to the working horse of mlrMBO.

1. define the objective function and its parameters by using the package **smoof**
2. generate an initial design
3. define a learner, i.e., the surrogate model
4. set up a MBO control object, which offers a load of options
5. finally start the optimization

This web page will provide you with an in-depth introduction on how to set the ``mbo()`` parameters depending on the desired kind of optimization.


# Objective Function


The first argument of ``mbo()`` is the name of the objective function created with ``makeSingleObjectiveFunction`` (or ``makeMultiObjectiveFunction``) from the package **smoof**. 

Throughout this tutorial we demonstrate the optimization of two simple functions: 

* ``objfun1``: The 5 dimensional ``ackley function`` which depends on 5 numeric parameters. ``objfun1`` should be minimized.

* ``objfun2``: A self-constructed sine und cosine combination, with two numeric and 1 discrete parameters. ``objfun2`` should be maximized. 


The 5 dimensional ``ackley function`` can be generated by the appropriate function of the **smoof** package

```{r}
objfun1 = makeAckleyFunction(5)
objfun1(c(1.8, 2.2, -4, 4, -5))
```

The self-constructed function can be built with ``makeSingleObjetiveFunction``. The ``par.set`` argument has to be a ParamSet object from the **ParamHelpers** package, which provides information about the parameters of the objective function and their constraints for optimization.
We define ``j`` in the interval [0,1] and ``k`` as an integer in {1, 2}. The Parameter ``method`` is discrete and can be either ``"a"`` or ``"b"``.
As stated, in this case we want to maximize the function. To do so we have to set ``minimize = FALSE``.

```{r}
foo = function(x) {
  j = x[[1]]
  k = x[[2]]
  method = x[[3]]
  perf = ifelse(method == "a", k * sin(j) + cos(j),
               sin(j) + k * cos(j))
  return(perf)
}

objfun2 = makeSingleObjectiveFunction(
  name = "example",
  fn = foo,
  par.set = makeParamSet(
    makeNumericParam("j", lower = 0,upper = 1),
    makeIntegerParam("k", lower = 1L, upper = 2L),
    makeDiscreteParam("method", values = c("a", "b"))
  ),
  has.simple.signature = FALSE,
  minimize = FALSE
)

objfun2(list(j = 0.5, k = 1L, method = "a"))
```


# Initial Design


The second argument of the ``mbo()`` function - ``design`` - is the initial design with default setting ``NULL``.

An easy (and recommended) way to create an initial design is to use the ``generateDesign`` function from the **ParamHelpers** package. If the default settings are used (i.e. ``design = NULL``) a Random Latin Hypercube ``lhs::randomLHS`` design is used with 4 times the number of parameters the objective function has. Other possibilities to generate designs are for example ``generateGridDesign`` and ``generateRandomDesign``. If special designs are desired (e.g., orthogonal designs), its interface has to be the same as the interface of the ``generateDesign`` objects. 
Particular attention has to be paid to the setting of the ``trafo`` attribute.

For ``objfun1`` and ``objfun2`` we create a slightly larger number of initial points than the default suggests. For ``objfun1`` we use Random Latin Hypercube sampling and for ``objfun2`` the Maximin Latin Hypercube sampling. The parameters of the sampling design have to be specified in a list and supplied via ``fun.args``.
```{r}
init.points1 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun1)))

set.seed(1)
design1 = generateDesign(n = init.points1, par.set = smoof::getParamSet(objfun1), fun = randomLHS, trafo = FALSE)

init.points2 = 5 * sum(ParamHelpers::getParamLengths(smoof::getParamSet(objfun2)))
design2 = generateDesign(n = init.points2, par.set = smoof::getParamSet(objfun2), fun.args = list(k = 3, dup = 4), 
                         fun = maximinLHS, trafo = FALSE)
```


# Surrogate Model


The attribute ``learner`` of the ``mbo()`` function allows us to choose an appropriate surrogate model for the parameter optimization. Different learners can easily created using the ``makeLearner`` function from the **mlr** package.
List of implemented learners can be seen using the ?learners command or on ``http://mlr-org.github.io/mlr-tutorial/release/html/integrated_learners/``
The choice of the surrogate model depends on the parameter set of the objective function.
While kriging models (gaussian processes) are advisable if all parameters are numeric, random forest models have be used if at one parameter is discrete. The default kriging model is from the **DiceKriging** package and uses the ``matern5_2``covariance kernel.
In our example we consider these two surrogate models:
``kriging`` for optimizing of ``objfun1``  and ``random forest`` for ``objfun2``.

```{r}
surr.km = makeLearner("regr.km", predict.type = "se", covtype = "matern3_2")
surr.rf = makeLearner("regr.randomForest")
```

Further modification of the learner (e.g., in order to get standard error prediction for design points) will be discussed und illustrated in the section "Experiments and Output".


# MBOControl


The ``MBOControl`` object is the control object for the fitting process and is created with ``makeMBOControl``. General control arguments can be set when creating it (e.g. the number of objectives (``n.objectives``), the number of points to propose in each iteration (``propose.points``), how the final point is proposed (``final.method``) etc.).
For further specifications additional control functions are used to define or change the settings of the object:  


## MBOControlInfill

With ``setMBOControlInfill`` a ``MBOControl`` object can be extended with infill criteria and infill optimizer options.

### Argument ``crit``

One of the most important issues is to define how the next design points in the sequential loops are chosen. 
5 different possibilities can be set via the ``crit`` argument in ``setMBOControlInfill``:

* ``mean``: mean response of the surrogate model
* ``ei``: expected improvement of the surrogate model
* ``aei``: augmented expected improvement, which is especially useful for the noisy functions
* ``eqi``: expected quantile improvement
* ``cb``: confidence bound, which is the additive combination of mean response and mean standard error estimation of the surrogate model (response - lambda * standard.error)

The parameters of the different criteria are set via further arguments (e.g. ``setMBOControlInfill`` for ``crit = cb``) 


### Argument ``opt``

The argument ``opt`` sets how the next point to evaluate should be proposed given an infill criterion. 
The possibilities are:

* ``focussearch``: Firstly a Latin Hypercube design of size ``opt.focussearch.points`` (default 10000) is sampled in the parameter space (by ``randomLHS``) and the design point with the best prediction of the infill criterion is determined. Then, the parameter space is shrunk around the best design point. This process is repeated ``opt.focussearch.maxit`` (default 5) times and the best seen value of the infill criterion is passed back.
* ```cmaes``: The optimal point is found with a covariance matrix adapting evolutionary strategy from the **cmeas** package. If the run fails, a random point is generated and a warning is given. Further control arguments can be provided in ``opt.cmaes.control`` as a list. 
* ``ea``: Use an evolutionary multiobjective optimization algorithm from the package **emoa** to determine the best point. The population size mu can be set by  ``opt.ea.mu`` (default value is 10). (mu+1) means that in each population only one child is generated using crossover und mutation operators. The parameters ``eta`` and ``p`` of the latter two operators can be adjusted via the attributes ``opt.ea.sbx.eta``, ``opt.ea.sbx.p``,``opt.ea.pm.eta`` and ``opt.ea.pm.p``. The default number of EA iterations is 500 and can be changed by ``opt.ea.maxit`` attribute.
* ``nsga2``: Use an non-dominated sorting genetic algorithm from the package **nsga2R** to determine the best point. This algorithm should be used for multi object optimization.

<!-- FIXME informations about "nsga2" -->

As all four infill optimization strategies do not guarantee  to find the global optimum, users can set the number of restarts by the ``opt.restarts`` argument (default value is 1).
After conducting the desired number of restarts the design point with the best infill criterion value is passed back to the MBO loop.

Please note that just ``focussearch`` optimizer is suitable for the case of factor parameters in the parameter set!


## setMBOControlTermination

With this control function different criteria to stop the fitting process can be specified. You can specify multiple different criteria and the first one that is met will stop the fitting. You can set a maximimum number of iterations with the ``iters`` argument, a maximum running time in seconds with ``time.budget``, a treshold for function evaluation (stop if a evaluation is better than a given value) and a maximum number of function evaluations. You can also easily create your own stopping condition(s).

## setMBOControlMultiPoint

This extends a MBO control object with options for multipoint proposal. Multipoint proposal means, that multiple infill points are suggested and evaluated, which is especially useful in parallel batch evaluation.

### Argument: method

Define the method used for multipoint proposalsm, currently 3 different methods are supported:

* ``cb``: Proposes multiple points by optimizing the confidence bound criterion ``propose.points`` times. Generally this works the same way as for the single point case, i.e. specify ``infill.opt``. The  lambda parameters are drawn from an exp(1)-distribution.
* ``multicrit``: Use a evolutionary multicriteria optimization. This is a (mu+1) type evolutionary algorithm and runs for ``multicrit.maxit`` generations. The population size is set to ``propose.points``.
* ``cl``: Proposes points by the constant liar strategy, which only makes sense if the confidence bound criterion is used as an infill criterion. In the first step the surrugate model is fitted based on the real data and the best point is calculated according to the regularly. Then, the function value of the best point is simply guessed by the worst seen function evaluation. This lie is used to update the model in order to propose the subsequent point. The procedure is applied until the number of best points achieves ``propose.points``.

## setMBOControlMultiFid

Add multi-fidelity options to the ``MBOControl`` control object. This is useful when certain parameters increase the performance as well as the calculation cost. The idea is to combine the optimization of fast fitting low-fidelity models and more accurate but expensive high-fidelity models. The parameter on which the fidelity depends on is specified as ``param`` and the order of the values to train the learner with in ``lvls``. The costs for the different levels can be specified or estimated by a model based on the execution time of the currently evaluated points.  

## setMBOControlMultiCrit

This adds multi-criteria optimization specific options to the control object. For details see the tutorial page on multi-criteria optimization.



The list of all attributes is provided in the software documentation.


```{r eval=FALSE}
control1 = makeMBOControl()
control1 = setMBOControlInfill(
  control = control1,
  crit = "ei",
  opt = "focussearch"
)
control1 = setMBOControlTermination(
  control = control1,
  iters = 10
)

control2 = makeMBOControl()
control2 = setMBOControlInfill(
  control = control2,
  crit = "mean",
  opt = "focussearch"
)
control2 = setMBOControlTermination(
  control = control2,
  iters = 10
)
```

# Experiments and Output


Now we will apply the mbo() function to optimize the two objective functions.

## Optimization of objfun1



```{r eval=FALSE}
mbo1 = mbo(objfun1, design = design1, learner = surr.km, control = control1, show.info = TRUE)
mbo1
```

The default output of mbo contains the best found parameter set and the optimzation path. The ``MBOResult`` object contains additional information, most importanty:

* x: the best point of the parameter space
* y: the associated best value of the objective function
* opt.path: optimization path. See **ParamHelpers** for further information.
* models: Depending on ``store.model.at`` in the ``MBOControl`` object, this contains zero, one or multiple surrogate models (default is to save the model generated after the last iteration).
* ...

We can also change some arguments of the ``MBOControl`` object and run ``mbo()`` again:

```{r eval=FALSE}
control1$infill.crit = "mean"
control1$iters = 5L
mbo1 = mbo(objfun1, design = design1, learner = surr.km, control = control1, show.info = FALSE)
mbo1$y
```


## Optimization of objfun2

Now let us use **mlrMBO** to optimize ``objfun2``, which contains one factor variable.
As we have already mentioned before, in case of factor variables only ``focussearch`` is suitable.
If we use ``mean`` as the infill criterion, any kind of model which can handle factors can be used (like regression trees, random forests, linear models and many others).

```{r eval=FALSE}
mbo2 = mbo(objfun2, design = design2, learner = learner_rf, control = control2, show.info = FALSE)
mbo2$y
```

If we want to use the expected improvement ``ei`` or (lower) confidence bound ``cb`` infill criteria,
the ``predict.type`` attribute of the learner has be set to ``se``. A list of regression learners which support it can be viewed by:

```{r eval=FALSE}
listLearners(type = "regr", properties = "se")
```


We modify the random forest to predict the standard error and optimize ``objfun2`` by the ``ei`` infill criterion.

```{r eval=FALSE}
learner_rf = makeLearner("regr.randomForest", predict.type = "se")
control2$infill.crit = "ei"
mbo2 = mbo(objfun2, design = design2, learner = learner_rf, control = control2, show.info = FALSE)
mbo2$y
```


Finally, if a learner, which does not support the ``se`` prediction type, should be applied for the optimization with the ``ei`` infill criterion, it possible to create a bagging model with the desired characteristics. For details on how to do it take a look at the [bagging section](https://mlr-org.github.io/mlr-tutorial/devel/html/bagging/index.html) in the ``mlr`` tutorial.

<!--
 TODO

1) noisy optimization example

2) mulicrit

3) multipoint
!-->
